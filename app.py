from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
import os
import pickle
import numpy as np
from transformers import BertTokenizer, BertForSequenceClassification
from huggingface_hub import hf_hub_download

# åˆ›å»º Flask åº”ç”¨
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# è®¾ç½®è®¾å¤‡
device = torch.device("cpu")

# ========================= ğŸŸ  åŠ è½½æ¨¡å‹ =========================
print("Downloading model from Hugging Face...")

# ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹æƒé‡ (bert_model.pt)
model_path = hf_hub_download(
    repo_id="LilithHu/bert-classifier",
    filename="bert_model.pt",
    local_dir="./model",  # ä¸‹è½½åˆ°æœ¬åœ° model æ–‡ä»¶å¤¹ï¼Œé˜²æ­¢é‡å¤ä¸‹è½½
    force_download=False  # è®¾ç½®ä¸º Falseï¼Œé¿å…æ¯æ¬¡é‡æ–°ä¸‹è½½
)

# åˆå§‹åŒ– Bert æ¨¡å‹
model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=12)
model.load_state_dict(torch.load(model_path, map_location=device))
model.to(device)
model.eval()
print("Model loaded successfully!")

# ========================= ğŸŸ¡ åŠ è½½æ ‡ç­¾ç¼–ç å™¨ =========================
# åŠ è½½ label_encoder.pklï¼ˆå‡è®¾å®ƒä»ç„¶ä½äºæœ¬åœ° model æ–‡ä»¶å¤¹ä¸­ï¼‰
print("Loading label encoder...")
model_dir = os.path.join(os.path.dirname(__file__), 'model')
with open(os.path.join(model_dir, 'label_encoder.pkl'), 'rb') as f:
    label_encoders = pickle.load(f)
print("Label encoder loaded successfully!")

# ========================= ğŸŸ¢ åŠ è½½ Tokenizer =========================
print("Loading tokenizer...")
tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")  # ä½¿ç”¨å…¬å¼€ tokenizer
print("Tokenizer loaded successfully!")


# ========================= ğŸŸ£ é¢„æµ‹å‡½æ•° =========================
def predict(text):
    try:
        # Tokenize input
        inputs = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')
        inputs = {key: val.to(device) for key, val in inputs.items()}

        # Get prediction
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            predicted_class = torch.argmax(logits, dim=1)
            predicted_class = predicted_class.cpu().numpy()

        # è½¬æ¢é¢„æµ‹ç»“æœä¸ºç±»åˆ«æ ‡ç­¾
        category = label_encoders['category'].classes_[predicted_class[0]]
        return category

    except Exception as e:
        print(f"Error in prediction: {str(e)}")
        raise e


# ========================= ğŸ”µ API è·¯ç”± =========================
@app.route('/predict', methods=['POST'])
def predict_route():
    try:
        data = request.get_json()
        text = data.get('text', None)
        if not text:
            return jsonify({"error": "Text input is required"}), 400

        category = predict(text)

        response = {
            "text": text,
            "predicted_category": category
        }

        return jsonify(response)

    except Exception as e:
        return jsonify({"error": str(e)}), 500


# ========================= ğŸŸ¡ å¯åŠ¨æœåŠ¡ =========================
if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8000))
    print(f"Starting Flask server on port {port}...")
    app.run(host='0.0.0.0', port=port)

